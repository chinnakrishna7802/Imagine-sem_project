{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "See Detailed code at https://github.com/aayush9753/ColorIt\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "Importing Libraries\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from models import *\n",
    "#from datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "from skimage import color\n",
    "from IPython import embed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Util\n",
    "def load_img(img_path):\n",
    "\tout_np = np.asarray(Image.open(img_path))\n",
    "\tif(out_np.ndim==2):\n",
    "\t\tout_np = np.tile(out_np[:,:,None],3)\n",
    "\treturn out_np\n",
    "\n",
    "def resize_img(img, HW=(256,256), resample=3):\n",
    "\treturn np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n",
    "\n",
    "def preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\n",
    "\t# return original size L and resized L as torch Tensors\n",
    "\timg_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
    "\t\n",
    "\timg_lab_orig = color.rgb2lab(img_rgb_orig)\n",
    "\timg_lab_rs = color.rgb2lab(img_rgb_rs)\n",
    "\n",
    "\timg_l_orig = img_lab_orig[:,:,0]\n",
    "\timg_l_rs = img_lab_rs[:,:,0]\n",
    "\n",
    "\ttens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n",
    "\ttens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n",
    "\n",
    "\treturn (tens_orig_l, tens_rs_l)\n",
    "\n",
    "def postprocess_tens_new(tens_orig_l, out_ab, mode='bilinear'):\n",
    "\t# tens_orig_l \tBatchsize x 1 x H_orig x W_orig\n",
    "\t# out_ab \t\tBatchsize x 2 x H x W\n",
    "    Batchsize = tens_orig_l.shape[0]\n",
    "\n",
    "    output_ = []\n",
    "    for i in range(Batchsize):\n",
    "        tens_orig_l_i = tens_orig_l[i][np.newaxis, :, :, :]\n",
    "        out_ab_i  = out_ab[i][np.newaxis, :, :, :]\n",
    "        HW_orig_i = tens_orig_l_i.shape[2:]\n",
    "        HW_i = out_ab_i.shape[2:]\n",
    "\n",
    "        # call resize function if needed\n",
    "        if(HW_orig_i[0]!=HW_i[0] or HW_orig_i[1]!=HW_i[1]):\n",
    "            out_ab_orig_i = F.interpolate(out_ab_i, size=HW_orig_i, mode='bilinear')\n",
    "        else:\n",
    "            out_ab_orig_i = out_ab_i\n",
    "\n",
    "        out_lab_orig_i = torch.cat((tens_orig_l_i, out_ab_orig_i), dim=1)\n",
    "        #output_.append(color.lab2rgb(out_lab_orig_i.data.cpu().numpy()[0,...].transpose((1,2,0))))\n",
    "        output_.append(color.lab2rgb(out_lab_orig_i.data.cpu().numpy()[0,...].transpose((1,2,0))).transpose((2,0,1)))\n",
    "    return np.array(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseColor(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(BaseColor, self).__init__()\n",
    "\n",
    "\t\tself.l_cent = 50.\n",
    "\t\tself.l_norm = 100.\n",
    "\t\tself.ab_norm = 110.\n",
    "\n",
    "\tdef normalize_l(self, in_l):\n",
    "\t\treturn (in_l-self.l_cent)/self.l_norm\n",
    "\n",
    "\tdef unnormalize_l(self, in_l):\n",
    "\t\treturn in_l*self.l_norm + self.l_cent\n",
    "\n",
    "\tdef normalize_ab(self, in_ab):\n",
    "\t\treturn in_ab/self.ab_norm\n",
    "\n",
    "\tdef unnormalize_ab(self, in_ab):\n",
    "\t\treturn in_ab*self.ab_norm\n",
    "\n",
    "\n",
    "class ECCVGenerator(BaseColor):\n",
    "    def __init__(self, norm_layer=nn.BatchNorm2d):\n",
    "        super(ECCVGenerator, self).__init__()\n",
    "\n",
    "        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        model1+=[norm_layer(64),]\n",
    "\n",
    "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        model2+=[norm_layer(128),]\n",
    "\n",
    "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[norm_layer(256),]\n",
    "\n",
    "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[norm_layer(512),]\n",
    "\n",
    "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[norm_layer(512),]\n",
    "\n",
    "        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[norm_layer(512),]\n",
    "\n",
    "        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[norm_layer(512),]\n",
    "\n",
    "        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "\n",
    "        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n",
    "\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "        self.model2 = nn.Sequential(*model2)\n",
    "        self.model3 = nn.Sequential(*model3)\n",
    "        self.model4 = nn.Sequential(*model4)\n",
    "        self.model5 = nn.Sequential(*model5)\n",
    "        self.model6 = nn.Sequential(*model6)\n",
    "        self.model7 = nn.Sequential(*model7)\n",
    "        self.model8 = nn.Sequential(*model8)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n",
    "        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
    "\n",
    "    def forward(self, input_l):\n",
    "        conv1_2 = self.model1(self.normalize_l(input_l))\n",
    "        conv2_2 = self.model2(conv1_2)\n",
    "        conv3_3 = self.model3(conv2_2)\n",
    "        conv4_3 = self.model4(conv3_3)\n",
    "        conv5_3 = self.model5(conv4_3)\n",
    "        conv6_3 = self.model6(conv5_3)\n",
    "        conv7_3 = self.model7(conv6_3)\n",
    "        conv8_3 = self.model8(conv7_3)\n",
    "        out_reg = self.model_out(self.softmax(conv8_3))\n",
    "\n",
    "        return self.unnormalize_ab(self.upsample4(out_reg))\n",
    "\n",
    "def eccv16(pretrained=True):\n",
    "\tmodel = ECCVGenerator()\n",
    "\tif(pretrained):\n",
    "\t\timport torch.utils.model_zoo as model_zoo\n",
    "\t\tmodel.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth',map_location='cpu',check_hash=True))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_cpu = 0\n",
    "out_channels = 3\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform): #, shape):\n",
    "        #height, width = shape\n",
    "        self.transform = transform\n",
    "        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        black_path = self.files[index % len(self.files)]\n",
    "        color_path = black_path.replace('black','color')\n",
    "        \n",
    "        img_black = np.asarray(Image.open(black_path))\n",
    "        if(img_black.ndim==2):\n",
    "            img_black = np.tile(img_black[:,:,None],3)\n",
    "        (tens_l_orig, tens_l_rs) = preprocess_img(img_black, HW=(400, 400))\n",
    "        #img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n",
    "        #img_black = self.transform(img_black)\n",
    "        \n",
    "        \n",
    "        img_color = Image.open(color_path)\n",
    "        img_color = self.transform(img_color)\n",
    "\n",
    "        return {\"black\": tens_l_rs.squeeze(0), 'orig': tens_l_orig.squeeze(0), \"color\": img_color}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "root = '../input/image-colorization-dataset/data/train_black/'\n",
    "transform = transforms.Compose(\n",
    "            [\n",
    "                #transforms.Resize((shape[0], shape[1]), Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(root, transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "\n",
    "i = 1\n",
    "img = data['black'][i].permute(1, 2, 0)\n",
    "print(img.shape)\n",
    "t = np.zeros((400, 400, 3))\n",
    "t[..., 0] = img[..., 0]\n",
    "t = color.lab2rgb(t)\n",
    "plt.imshow(t)\n",
    "plt.show()\n",
    "\n",
    "img = data['orig'][i].permute(1, 2, 0)\n",
    "print(img.shape)\n",
    "t = np.zeros((400, 400, 3))\n",
    "t[..., 0] = img[..., 0]\n",
    "t = color.lab2rgb(t)\n",
    "plt.imshow(t)\n",
    "plt.show()\n",
    "\n",
    "print(img.shape)\n",
    "img = data['color'][i].permute(1, 2, 0)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color_ecv(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(color_ecv, self).__init__()\n",
    "        \n",
    "        self.model = eccv16(pretrained=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ecv_output = self.model(x)\n",
    "        return ecv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 55\n",
    "n_epochs= 60\n",
    "\n",
    "lr = 0.0001\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "in_channels = 1\n",
    "out_channels = 3\n",
    "sample_interval = 100\n",
    "checkpoint_interval = 1\n",
    "\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (400, 400)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = color_ecv(in_channels = 3)\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_content = torch.nn.L1Loss()\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    feature_extractor = feature_extractor.cuda()\n",
    "    criterion_GAN = criterion_GAN.cuda()\n",
    "    criterion_content = criterion_content.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_state_dict(torch.load(\"generator_59.pth\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
