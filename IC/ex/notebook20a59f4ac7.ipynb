{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "combined-country",
   "metadata": {
    "papermill": {
     "duration": 0.015136,
     "end_time": "2023-02-03T05:05:10.906211",
     "exception": false,
     "start_time": "2023-02-03T05:05:10.891075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# See Detailed code at https://github.com/aayush9753/ColorIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "national-brother",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:10.939593Z",
     "iopub.status.busy": "2023-02-03T05:05:10.938118Z",
     "iopub.status.idle": "2023-02-03T05:05:10.946633Z",
     "shell.execute_reply": "2023-02-03T05:05:10.946144Z",
     "shell.execute_reply.started": "2023-02-01T17:28:30.109084Z"
    },
    "papermill": {
     "duration": 0.026477,
     "end_time": "2023-02-03T05:05:10.946782",
     "exception": false,
     "start_time": "2023-02-03T05:05:10.920305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-arrest",
   "metadata": {
    "papermill": {
     "duration": 0.013375,
     "end_time": "2023-02-03T05:05:10.973797",
     "exception": false,
     "start_time": "2023-02-03T05:05:10.960422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "breathing-registration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:11.007722Z",
     "iopub.status.busy": "2023-02-03T05:05:11.006985Z",
     "iopub.status.idle": "2023-02-03T05:05:13.231293Z",
     "shell.execute_reply": "2023-02-03T05:05:13.230545Z",
     "shell.execute_reply.started": "2023-02-01T17:28:30.121988Z"
    },
    "papermill": {
     "duration": 2.243477,
     "end_time": "2023-02-03T05:05:13.231432",
     "exception": false,
     "start_time": "2023-02-03T05:05:10.987955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from models import *\n",
    "#from datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "from skimage import color\n",
    "from IPython import embed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-outreach",
   "metadata": {
    "papermill": {
     "duration": 0.014229,
     "end_time": "2023-02-03T05:05:13.259731",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.245502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "pleased-notebook",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:13.300240Z",
     "iopub.status.busy": "2023-02-03T05:05:13.299365Z",
     "iopub.status.idle": "2023-02-03T05:05:13.302244Z",
     "shell.execute_reply": "2023-02-03T05:05:13.301774Z",
     "shell.execute_reply.started": "2023-02-01T17:28:31.933553Z"
    },
    "papermill": {
     "duration": 0.028303,
     "end_time": "2023-02-03T05:05:13.302367",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.274064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "\tout_np = np.asarray(Image.open(img_path))\n",
    "\tif(out_np.ndim==2):\n",
    "\t\tout_np = np.tile(out_np[:,:,None],3)\n",
    "\treturn out_np\n",
    "\n",
    "def resize_img(img, HW=(256,256), resample=3):\n",
    "\treturn np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n",
    "\n",
    "def preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\n",
    "\t# return original size L and resized L as torch Tensors\n",
    "\timg_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
    "\t\n",
    "\timg_lab_orig = color.rgb2lab(img_rgb_orig)\n",
    "\timg_lab_rs = color.rgb2lab(img_rgb_rs)\n",
    "\n",
    "\timg_l_orig = img_lab_orig[:,:,0]\n",
    "\timg_l_rs = img_lab_rs[:,:,0]\n",
    "\n",
    "\ttens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n",
    "\ttens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n",
    "\n",
    "\treturn (tens_orig_l, tens_rs_l)\n",
    "\n",
    "def postprocess_tens_new(tens_orig_l, out_ab, mode='bilinear'):\n",
    "\t# tens_orig_l \tBatchsize x 1 x H_orig x W_orig\n",
    "\t# out_ab \t\tBatchsize x 2 x H x W\n",
    "    Batchsize = tens_orig_l.shape[0]\n",
    "\n",
    "    output_ = []\n",
    "    for i in range(Batchsize):\n",
    "        tens_orig_l_i = tens_orig_l[i][np.newaxis, :, :, :]\n",
    "        out_ab_i  = out_ab[i][np.newaxis, :, :, :]\n",
    "        HW_orig_i = tens_orig_l_i.shape[2:]\n",
    "        HW_i = out_ab_i.shape[2:]\n",
    "\n",
    "        # call resize function if needed\n",
    "        if(HW_orig_i[0]!=HW_i[0] or HW_orig_i[1]!=HW_i[1]):\n",
    "            out_ab_orig_i = F.interpolate(out_ab_i, size=HW_orig_i, mode='bilinear')\n",
    "        else:\n",
    "            out_ab_orig_i = out_ab_i\n",
    "\n",
    "        out_lab_orig_i = torch.cat((tens_orig_l_i, out_ab_orig_i), dim=1)\n",
    "        #output_.append(color.lab2rgb(out_lab_orig_i.data.cpu().numpy()[0,...].transpose((1,2,0))))\n",
    "        output_.append(color.lab2rgb(out_lab_orig_i.data.cpu().numpy()[0,...].transpose((1,2,0))).transpose((2,0,1)))\n",
    "    return np.array(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-mainstream",
   "metadata": {
    "papermill": {
     "duration": 0.015401,
     "end_time": "2023-02-03T05:05:13.333484",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.318083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Models\n",
    "## ECV16\n",
    "This Model is based on the paper Colorful Image Colorization https://arxiv.org/abs/1603.08511 which we will train using a GAN with the ECV Model (A normal residual convnet model proposed in the paper) as the generator and a discriminator that we will design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "searching-royalty",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:13.379621Z",
     "iopub.status.busy": "2023-02-03T05:05:13.374547Z",
     "iopub.status.idle": "2023-02-03T05:05:13.399323Z",
     "shell.execute_reply": "2023-02-03T05:05:13.398814Z",
     "shell.execute_reply.started": "2023-02-01T17:28:31.947720Z"
    },
    "papermill": {
     "duration": 0.049959,
     "end_time": "2023-02-03T05:05:13.399437",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.349478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseColor(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(BaseColor, self).__init__()\n",
    "\n",
    "\t\tself.l_cent = 50.\n",
    "\t\tself.l_norm = 100.\n",
    "\t\tself.ab_norm = 110.\n",
    "\n",
    "\tdef normalize_l(self, in_l):\n",
    "\t\treturn (in_l-self.l_cent)/self.l_norm\n",
    "\n",
    "\tdef unnormalize_l(self, in_l):\n",
    "\t\treturn in_l*self.l_norm + self.l_cent\n",
    "\n",
    "\tdef normalize_ab(self, in_ab):\n",
    "\t\treturn in_ab/self.ab_norm\n",
    "\n",
    "\tdef unnormalize_ab(self, in_ab):\n",
    "\t\treturn in_ab*self.ab_norm\n",
    "\n",
    "\n",
    "class ECCVGenerator(BaseColor):\n",
    "    def __init__(self, norm_layer=nn.BatchNorm2d):\n",
    "        super(ECCVGenerator, self).__init__()\n",
    "\n",
    "        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model1+=[nn.ReLU(True),]\n",
    "        model1+=[norm_layer(64),]\n",
    "\n",
    "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model2+=[nn.ReLU(True),]\n",
    "        model2+=[norm_layer(128),]\n",
    "\n",
    "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n",
    "        model3+=[nn.ReLU(True),]\n",
    "        model3+=[norm_layer(256),]\n",
    "\n",
    "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model4+=[nn.ReLU(True),]\n",
    "        model4+=[norm_layer(512),]\n",
    "\n",
    "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model5+=[nn.ReLU(True),]\n",
    "        model5+=[norm_layer(512),]\n",
    "\n",
    "        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
    "        model6+=[nn.ReLU(True),]\n",
    "        model6+=[norm_layer(512),]\n",
    "\n",
    "        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model7+=[nn.ReLU(True),]\n",
    "        model7+=[norm_layer(512),]\n",
    "\n",
    "        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
    "        model8+=[nn.ReLU(True),]\n",
    "\n",
    "        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n",
    "\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "        self.model2 = nn.Sequential(*model2)\n",
    "        self.model3 = nn.Sequential(*model3)\n",
    "        self.model4 = nn.Sequential(*model4)\n",
    "        self.model5 = nn.Sequential(*model5)\n",
    "        self.model6 = nn.Sequential(*model6)\n",
    "        self.model7 = nn.Sequential(*model7)\n",
    "        self.model8 = nn.Sequential(*model8)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n",
    "        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
    "\n",
    "    def forward(self, input_l):\n",
    "        conv1_2 = self.model1(self.normalize_l(input_l))\n",
    "        conv2_2 = self.model2(conv1_2)\n",
    "        conv3_3 = self.model3(conv2_2)\n",
    "        conv4_3 = self.model4(conv3_3)\n",
    "        conv5_3 = self.model5(conv4_3)\n",
    "        conv6_3 = self.model6(conv5_3)\n",
    "        conv7_3 = self.model7(conv6_3)\n",
    "        conv8_3 = self.model8(conv7_3)\n",
    "        out_reg = self.model_out(self.softmax(conv8_3))\n",
    "\n",
    "        return self.unnormalize_ab(self.upsample4(out_reg))\n",
    "\n",
    "def eccv16(pretrained=True):\n",
    "\tmodel = ECCVGenerator()\n",
    "\tif(pretrained):\n",
    "\t\timport torch.utils.model_zoo as model_zoo\n",
    "\t\tmodel.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth',map_location='cpu',check_hash=True))\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-clark",
   "metadata": {
    "papermill": {
     "duration": 0.015869,
     "end_time": "2023-02-03T05:05:13.430684",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.414815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Extractor\n",
    "This is a noval approach to compare to images and use that comparison as a loss to train a model.\n",
    "We will use a pre-trained VGG19 model. Two images are passed in it and the activations of the 18th layers are taken for both the images and then this activations are used to calculate the loss which can be calculated using RMSE, MSE etc between the two activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "maritime-burden",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:13.463842Z",
     "iopub.status.busy": "2023-02-03T05:05:13.463177Z",
     "iopub.status.idle": "2023-02-03T05:05:13.466027Z",
     "shell.execute_reply": "2023-02-03T05:05:13.465572Z",
     "shell.execute_reply.started": "2023-02-01T17:28:31.983758Z"
    },
    "papermill": {
     "duration": 0.02078,
     "end_time": "2023-02-03T05:05:13.466141",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.445361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-headquarters",
   "metadata": {
    "papermill": {
     "duration": 0.013925,
     "end_time": "2023-02-03T05:05:13.493801",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.479876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Discriminator\n",
    "Takes an image as input and converts it into a single no. after passing through several convolutional blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "minimal-appreciation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:13.561125Z",
     "iopub.status.busy": "2023-02-03T05:05:13.560304Z",
     "iopub.status.idle": "2023-02-03T05:05:13.570130Z",
     "shell.execute_reply": "2023-02-03T05:05:13.571032Z",
     "shell.execute_reply.started": "2023-02-01T17:28:31.998771Z"
    },
    "papermill": {
     "duration": 0.049399,
     "end_time": "2023-02-03T05:05:13.571176",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.521777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, first_block=False):\n",
    "            layers = []\n",
    "            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
    "            if not first_block:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-attachment",
   "metadata": {
    "papermill": {
     "duration": 0.02841,
     "end_time": "2023-02-03T05:05:13.626491",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.598081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "executive-musician",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:13.678874Z",
     "iopub.status.busy": "2023-02-03T05:05:13.678117Z",
     "iopub.status.idle": "2023-02-03T05:05:14.353736Z",
     "shell.execute_reply": "2023-02-03T05:05:14.353243Z",
     "shell.execute_reply.started": "2023-02-01T17:33:01.648927Z"
    },
    "papermill": {
     "duration": 0.705199,
     "end_time": "2023-02-03T05:05:14.353872",
     "exception": false,
     "start_time": "2023-02-03T05:05:13.648673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m root \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../input/image-colorization-dataset/data/train_black/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m             [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                 \u001b[39m#transforms.Resize((shape[0], shape[1]), Image.BICUBIC),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m             ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     ImageDataset(root, transform),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     num_workers\u001b[39m=\u001b[39mn_cpu,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:277\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# map-style\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[39mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 277\u001b[0m         sampler \u001b[39m=\u001b[39m RandomSampler(dataset, generator\u001b[39m=\u001b[39;49mgenerator)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/sampler.py:97\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "n_cpu = 0\n",
    "out_channels = 3\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform): #, shape):\n",
    "        #height, width = shape\n",
    "        self.transform = transform\n",
    "        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        black_path = self.files[index % len(self.files)]\n",
    "        color_path = black_path.replace('black','color')\n",
    "        \n",
    "        img_black = np.asarray(Image.open(black_path))\n",
    "        if(img_black.ndim==2):\n",
    "            img_black = np.tile(img_black[:,:,None],3)\n",
    "        (tens_l_orig, tens_l_rs) = preprocess_img(img_black, HW=(400, 400))\n",
    "        #img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n",
    "        #img_black = self.transform(img_black)\n",
    "        \n",
    "        \n",
    "        img_color = Image.open(color_path)\n",
    "        img_color = self.transform(img_color)\n",
    "\n",
    "        return {\"black\": tens_l_rs.squeeze(0), 'orig': tens_l_orig.squeeze(0), \"color\": img_color}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "root = '../input/image-colorization-dataset/data/train_black/'\n",
    "transform = transforms.Compose(\n",
    "            [\n",
    "                #transforms.Resize((shape[0], shape[1]), Image.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize(mean, std),\n",
    "            ]\n",
    "        )\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(root, transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "tough-renaissance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:14.390187Z",
     "iopub.status.busy": "2023-02-03T05:05:14.389541Z",
     "iopub.status.idle": "2023-02-03T05:05:15.683079Z",
     "shell.execute_reply": "2023-02-03T05:05:15.682583Z"
    },
    "papermill": {
     "duration": 1.315128,
     "end_time": "2023-02-03T05:05:15.683238",
     "exception": false,
     "start_time": "2023-02-03T05:05:14.368110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(dataloader)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m dataiter\u001b[39m.\u001b[39mnext()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "\n",
    "i = 1\n",
    "img = data['black'][i].permute(1, 2, 0)\n",
    "print(img.shape)\n",
    "t = np.zeros((400, 400, 3))\n",
    "t[..., 0] = img[..., 0]\n",
    "t = color.lab2rgb(t)\n",
    "plt.imshow(t)\n",
    "plt.show()\n",
    "\n",
    "img = data['orig'][i].permute(1, 2, 0)\n",
    "print(img.shape)\n",
    "t = np.zeros((400, 400, 3))\n",
    "t[..., 0] = img[..., 0]\n",
    "t = color.lab2rgb(t)\n",
    "plt.imshow(t)\n",
    "plt.show()\n",
    "\n",
    "print(img.shape)\n",
    "img = data['color'][i].permute(1, 2, 0)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-allowance",
   "metadata": {
    "papermill": {
     "duration": 0.019573,
     "end_time": "2023-02-03T05:05:15.723756",
     "exception": false,
     "start_time": "2023-02-03T05:05:15.704183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialising Models and Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "hungry-guess",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:15.768227Z",
     "iopub.status.busy": "2023-02-03T05:05:15.767716Z",
     "iopub.status.idle": "2023-02-03T05:05:15.771536Z",
     "shell.execute_reply": "2023-02-03T05:05:15.771115Z"
    },
    "papermill": {
     "duration": 0.028119,
     "end_time": "2023-02-03T05:05:15.771661",
     "exception": false,
     "start_time": "2023-02-03T05:05:15.743542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class color_ecv(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(color_ecv, self).__init__()\n",
    "        \n",
    "        self.model = eccv16(pretrained=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ecv_output = self.model(x)\n",
    "        return ecv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "listed-military",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:15.814529Z",
     "iopub.status.busy": "2023-02-03T05:05:15.813947Z",
     "iopub.status.idle": "2023-02-03T05:05:15.817182Z",
     "shell.execute_reply": "2023-02-03T05:05:15.816766Z"
    },
    "papermill": {
     "duration": 0.025998,
     "end_time": "2023-02-03T05:05:15.817289",
     "exception": false,
     "start_time": "2023-02-03T05:05:15.791291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"colorit_gan/images\", exist_ok=True)\n",
    "os.makedirs(\"colorit_gan/saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "proud-extreme",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:15.968685Z",
     "iopub.status.busy": "2023-02-03T05:05:15.967865Z",
     "iopub.status.idle": "2023-02-03T05:05:15.970054Z",
     "shell.execute_reply": "2023-02-03T05:05:15.970420Z"
    },
    "papermill": {
     "duration": 0.133565,
     "end_time": "2023-02-03T05:05:15.970568",
     "exception": false,
     "start_time": "2023-02-03T05:05:15.837003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_epoch = 55\n",
    "n_epochs= 60\n",
    "\n",
    "lr = 0.0001\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "in_channels = 1\n",
    "out_channels = 3\n",
    "sample_interval = 100\n",
    "checkpoint_interval = 1\n",
    "\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "short-guest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:16.023150Z",
     "iopub.status.busy": "2023-02-03T05:05:16.022543Z",
     "iopub.status.idle": "2023-02-03T05:05:53.966419Z",
     "shell.execute_reply": "2023-02-03T05:05:53.965945Z"
    },
    "papermill": {
     "duration": 37.975198,
     "end_time": "2023-02-03T05:05:53.966557",
     "exception": false,
     "start_time": "2023-02-03T05:05:15.991359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape = (400, 400)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = color_ecv(in_channels = 3)\n",
    "discriminator = Discriminator(input_shape=(out_channels, *shape))\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_content = torch.nn.L1Loss()\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    feature_extractor = feature_extractor.cuda()\n",
    "    criterion_GAN = criterion_GAN.cuda()\n",
    "    criterion_content = criterion_content.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bff43808",
   "metadata": {},
   "source": [
    "65,70,75,80,85,90\n",
    "64,69,74,79,84,89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "proved-problem",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T05:05:54.013460Z",
     "iopub.status.busy": "2023-02-03T05:05:54.012933Z",
     "iopub.status.idle": "2023-02-03T05:05:55.413482Z",
     "shell.execute_reply": "2023-02-03T05:05:55.413009Z"
    },
    "papermill": {
     "duration": 1.425939,
     "end_time": "2023-02-03T05:05:55.413644",
     "exception": false,
     "start_time": "2023-02-03T05:05:53.987705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if start_epoch != 0:\n",
    "    # Load pretrained models\n",
    "generator.load_state_dict(torch.load(\"generator_59.pth\",map_location=torch.device('cpu')))\n",
    "    # discriminator.load_state_dict(torch.load(\"/kaggle/input/colourit-saved-model-e55/discriminator_\"+str(start_epoch-1)+\".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7119f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color_ecv(\n",
       "  (model): ECCVGenerator(\n",
       "    (model1): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model6): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model7): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model8): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (upsample4): Upsample(scale_factor=4.0, mode=bilinear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "48237e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.color_ecv"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d09965ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load(path,shape):\n",
    "    img= cv2.imread(path)\n",
    "    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img= cv2.resize(img, shape)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b917a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d204c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for folder in glob.glob(path+ str('/*')):\n",
    "        for img_path in glob.glob(folder+ str('/*')):\n",
    "            if folder == os.path.join(path, 'test_black'):\n",
    "                X.append(load(img_path, (400, 400)))\n",
    "            elif folder == os.path.join(path, 'test_color'):\n",
    "                Y.append(load(img_path, (400,400)))\n",
    "    X= np.array(X)\n",
    "    Y= np.array(Y)\n",
    "    return X/255.0, Y/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbb1debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_black , test_color = get_data(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f73b4373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 400, 400, 3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_black.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad8226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 400, 400, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35165a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color_ecv(\n",
       "  (model): ECCVGenerator(\n",
       "    (model1): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model6): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model7): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (model8): Sequential(\n",
       "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (upsample4): Upsample(scale_factor=4.0, mode=bilinear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1817f1ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 400, 400, 3]' is invalid for input of size 4800000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m Variable\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m Tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mFloatTensor \u001b[39mif\u001b[39;00m cuda \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mTensor\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(test_black)\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m,\u001b[39m400\u001b[39m,\u001b[39m400\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m imgs_black \u001b[39m=\u001b[39m Variable(\u001b[39minput\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m imgs_black_orig \u001b[39m=\u001b[39m Variable(\u001b[39minput\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 400, 400, 3]' is invalid for input of size 4800000"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "input = torch.from_numpy(test_black)#.reshape((1,400,400,3))\n",
    "\n",
    "imgs_black = Variable(input)\n",
    "imgs_black_orig = Variable(input)\n",
    "imgs_color = Variable(torch.from_numpy(test_color[0]).reshape((1,400,400,3)))\n",
    "\n",
    "gen_ab = generator(input)\n",
    "gen_color = postprocess_tens_new(input, gen_ab)\n",
    "\n",
    "gen_color = make_grid(gen_color.detach(), nrow=1, normalize=True)\n",
    "imgs_black_orig = make_grid(imgs_black_orig, nrow=1, normalize=True)\n",
    "img_grid = torch.cat((imgs_black_orig, gen_color), -1)\n",
    "save_image(img_grid, \"output.png\", normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c916a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.from_numpy(test_black[0]).reshape((1,400,400,3))\n",
    "type(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f356f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b1c7f83f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m \u001b[39mimport\u001b[39;00m color\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m img \u001b[39m=\u001b[39m test_black[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose((\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m img \u001b[39m=\u001b[39m test_black[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose((\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/notebook20a59f4ac7.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# permuted_tensor = test_black[0].permute((1, 0))\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "from skimage import color\n",
    "\n",
    "img = test_black[0].permute(1, 2, 0)\n",
    "print(img.shape)\n",
    "t = np.zeros((400, 400, 3))\n",
    "t[..., 0] = img[..., 0]\n",
    "t = color.lab2rgb(t)\n",
    "plt.imshow(t)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11393.965007,
   "end_time": "2023-02-03T08:14:58.228937",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-03T05:05:04.263930",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03e93882f79c4ea0a2f4df147e1e2b1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "07c822020f0448d18af04d33661aa933": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_205bc7ce504045a7ae256092fc123927",
        "IPY_MODEL_0c52379fc1b94f37b0458ae341002994",
        "IPY_MODEL_68b6a997224c4dbf93d91a5c7b810762"
       ],
       "layout": "IPY_MODEL_dd3468f96dec4df78d4e624f2fc6926f"
      }
     },
     "0c52379fc1b94f37b0458ae341002994": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_03e93882f79c4ea0a2f4df147e1e2b1a",
       "max": 128976165,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4f1282b175ab4f87b77752b0c22b31d8",
       "value": 128976165
      }
     },
     "0fd4cb7d09054546b6024706bb7f810f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "205bc7ce504045a7ae256092fc123927": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3477c156654242e99e6dd647dde84fef",
       "placeholder": "​",
       "style": "IPY_MODEL_2533a444a07942d1b6b33fda1977c715",
       "value": "100%"
      }
     },
     "2533a444a07942d1b6b33fda1977c715": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2c2a242873144696a6ff037c77c73c86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fc544f2682ff42fb850ed63d75087ba1",
       "max": 574673361,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a1c5ab2345a944ab97ea75cb11ac6d1b",
       "value": 574673361
      }
     },
     "3477c156654242e99e6dd647dde84fef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "468e57a1ac5540b9bf618ef15e97725e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a835f008e0334053ac794f533c1dfb1a",
       "placeholder": "​",
       "style": "IPY_MODEL_50c179d8ddc94096a678d717e3951826",
       "value": "100%"
      }
     },
     "4f1282b175ab4f87b77752b0c22b31d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "50c179d8ddc94096a678d717e3951826": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "545a6cfa743d477b9ecbf628c891ea41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_468e57a1ac5540b9bf618ef15e97725e",
        "IPY_MODEL_2c2a242873144696a6ff037c77c73c86",
        "IPY_MODEL_7eb544aa41b74c39a82b138dccff3be6"
       ],
       "layout": "IPY_MODEL_0fd4cb7d09054546b6024706bb7f810f"
      }
     },
     "68b6a997224c4dbf93d91a5c7b810762": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_737818dcf31a4d798fa57726da9ef0bd",
       "placeholder": "​",
       "style": "IPY_MODEL_6a52b058cb244049b0bfb57b3acc1c26",
       "value": " 123M/123M [00:03&lt;00:00, 39.1MB/s]"
      }
     },
     "6a52b058cb244049b0bfb57b3acc1c26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "737818dcf31a4d798fa57726da9ef0bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7eb544aa41b74c39a82b138dccff3be6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f0d633997d5442179e8447421b99a9c0",
       "placeholder": "​",
       "style": "IPY_MODEL_7ebd5a9418884e9099d30b5197785a59",
       "value": " 548M/548M [00:25&lt;00:00, 23.8MB/s]"
      }
     },
     "7ebd5a9418884e9099d30b5197785a59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a1c5ab2345a944ab97ea75cb11ac6d1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a835f008e0334053ac794f533c1dfb1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd3468f96dec4df78d4e624f2fc6926f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0d633997d5442179e8447421b99a9c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc544f2682ff42fb850ed63d75087ba1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
