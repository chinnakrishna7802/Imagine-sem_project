{"cells":[{"cell_type":"markdown","metadata":{},"source":["# See Detailed code at https://github.com/aayush9753/ColorIt"]},{"cell_type":"code","execution_count":22,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-09-07T17:03:23.305733Z","iopub.status.idle":"2021-09-07T17:03:23.306388Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:38.592211Z","iopub.status.busy":"2021-09-07T17:03:38.591857Z","iopub.status.idle":"2021-09-07T17:03:40.391267Z","shell.execute_reply":"2021-09-07T17:03:40.390386Z","shell.execute_reply.started":"2021-09-07T17:03:38.592180Z"},"trusted":true},"outputs":[],"source":["import glob\n","import random\n","import os\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import torchvision.transforms as transforms\n","from torch import nn\n","from matplotlib import pyplot as plt\n","\n","#from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n","\n","import argparse\n","import os\n","import numpy as np\n","import math\n","import itertools\n","import sys\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image, make_grid\n","\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","\n","#from models import *\n","#from datasets import *\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","\n","from torchvision.models import vgg19\n","\n","from skimage import color\n","from IPython import embed"]},{"cell_type":"markdown","metadata":{},"source":["## Util"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:43.411614Z","iopub.status.busy":"2021-09-07T17:03:43.411277Z","iopub.status.idle":"2021-09-07T17:03:43.424540Z","shell.execute_reply":"2021-09-07T17:03:43.423494Z","shell.execute_reply.started":"2021-09-07T17:03:43.411584Z"},"trusted":true},"outputs":[],"source":["def load_img(img_path):\n","\tout_np = np.asarray(Image.open(img_path))\n","\tif(out_np.ndim==2):\n","\t\tout_np = np.tile(out_np[:,:,None],3)\n","\treturn out_np\n","\n","def resize_img(img, HW=(256,256), resample=3):\n","\treturn np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n","\n","def preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\n","\t# return original size L and resized L as torch Tensors\n","\timg_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n","\t\n","\timg_lab_orig = color.rgb2lab(img_rgb_orig)\n","\timg_lab_rs = color.rgb2lab(img_rgb_rs)\n","\n","\timg_l_orig = img_lab_orig[:,:,0]\n","\timg_l_rs = img_lab_rs[:,:,0]\n","\n","\ttens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n","\ttens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n","\n","\treturn (tens_orig_l, tens_rs_l)\n","\n","def postprocess_tens_new(tens_orig_l, out_ab, mode='bilinear'):\n","\t# tens_orig_l \tBatchsize x 1 x H_orig x W_orig\n","\t# out_ab \t\tBatchsize x 2 x H x W\n","    Batchsize = tens_orig_l.shape[0]\n","\n","    output_ = []\n","    for i in range(Batchsize):\n","        tens_orig_l_i = tens_orig_l[i][np.newaxis, :, :, :]\n","        out_ab_i  = out_ab[i][np.newaxis, :, :, :]\n","        HW_orig_i = tens_orig_l_i.shape[2:]\n","        HW_i = out_ab_i.shape[2:]\n","\n","        # call resize function if needed\n","        if(HW_orig_i[0]!=HW_i[0] or HW_orig_i[1]!=HW_i[1]):\n","            out_ab_orig_i = F.interpolate(out_ab_i, size=HW_orig_i, mode='bilinear')\n","        else:\n","            out_ab_orig_i = out_ab_i\n","\n","        out_lab_orig_i = torch.cat((tens_orig_l_i, out_ab_orig_i), dim=1)\n","        #output_.append(color.lab2rgb(out_lab_orig_i.data.cpu().numpy()[0,...].transpose((1,2,0))))\n","        output_.append(color.lab2rgb(out_lab_orig_i.data.cpu().numpy()[0,...].transpose((1,2,0))).transpose((2,0,1)))\n","    return np.array(output_)"]},{"cell_type":"markdown","metadata":{},"source":["# Models\n","## ECV16\n","This Model is based on the paper Colorful Image Colorization https://arxiv.org/abs/1603.08511 which we will train using a GAN with the ECV Model (A normal residual convnet model proposed in the paper) as the generator and a discriminator that we will design."]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:44.386173Z","iopub.status.busy":"2021-09-07T17:03:44.385798Z","iopub.status.idle":"2021-09-07T17:03:44.419085Z","shell.execute_reply":"2021-09-07T17:03:44.417690Z","shell.execute_reply.started":"2021-09-07T17:03:44.386139Z"},"trusted":true},"outputs":[],"source":["class BaseColor(nn.Module):\n","\tdef __init__(self):\n","\t\tsuper(BaseColor, self).__init__()\n","\n","\t\tself.l_cent = 50.\n","\t\tself.l_norm = 100.\n","\t\tself.ab_norm = 110.\n","\n","\tdef normalize_l(self, in_l):\n","\t\treturn (in_l-self.l_cent)/self.l_norm\n","\n","\tdef unnormalize_l(self, in_l):\n","\t\treturn in_l*self.l_norm + self.l_cent\n","\n","\tdef normalize_ab(self, in_ab):\n","\t\treturn in_ab/self.ab_norm\n","\n","\tdef unnormalize_ab(self, in_ab):\n","\t\treturn in_ab*self.ab_norm\n","\n","\n","class ECCVGenerator(BaseColor):\n","    def __init__(self, norm_layer=nn.BatchNorm2d):\n","        super(ECCVGenerator, self).__init__()\n","\n","        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model1+=[nn.ReLU(True),]\n","        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n","        model1+=[nn.ReLU(True),]\n","        model1+=[norm_layer(64),]\n","\n","        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model2+=[nn.ReLU(True),]\n","        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n","        model2+=[nn.ReLU(True),]\n","        model2+=[norm_layer(128),]\n","\n","        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model3+=[nn.ReLU(True),]\n","        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model3+=[nn.ReLU(True),]\n","        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n","        model3+=[nn.ReLU(True),]\n","        model3+=[norm_layer(256),]\n","\n","        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model4+=[nn.ReLU(True),]\n","        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model4+=[nn.ReLU(True),]\n","        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model4+=[nn.ReLU(True),]\n","        model4+=[norm_layer(512),]\n","\n","        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n","        model5+=[nn.ReLU(True),]\n","        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n","        model5+=[nn.ReLU(True),]\n","        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n","        model5+=[nn.ReLU(True),]\n","        model5+=[norm_layer(512),]\n","\n","        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n","        model6+=[nn.ReLU(True),]\n","        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n","        model6+=[nn.ReLU(True),]\n","        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n","        model6+=[nn.ReLU(True),]\n","        model6+=[norm_layer(512),]\n","\n","        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model7+=[nn.ReLU(True),]\n","        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model7+=[nn.ReLU(True),]\n","        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model7+=[nn.ReLU(True),]\n","        model7+=[norm_layer(512),]\n","\n","        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n","        model8+=[nn.ReLU(True),]\n","        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model8+=[nn.ReLU(True),]\n","        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n","        model8+=[nn.ReLU(True),]\n","\n","        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n","\n","        self.model1 = nn.Sequential(*model1)\n","        self.model2 = nn.Sequential(*model2)\n","        self.model3 = nn.Sequential(*model3)\n","        self.model4 = nn.Sequential(*model4)\n","        self.model5 = nn.Sequential(*model5)\n","        self.model6 = nn.Sequential(*model6)\n","        self.model7 = nn.Sequential(*model7)\n","        self.model8 = nn.Sequential(*model8)\n","\n","        self.softmax = nn.Softmax(dim=1)\n","        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n","        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n","\n","    def forward(self, input_l):\n","        conv1_2 = self.model1(self.normalize_l(input_l))\n","        conv2_2 = self.model2(conv1_2)\n","        conv3_3 = self.model3(conv2_2)\n","        conv4_3 = self.model4(conv3_3)\n","        conv5_3 = self.model5(conv4_3)\n","        conv6_3 = self.model6(conv5_3)\n","        conv7_3 = self.model7(conv6_3)\n","        conv8_3 = self.model8(conv7_3)\n","        out_reg = self.model_out(self.softmax(conv8_3))\n","\n","        return self.unnormalize_ab(self.upsample4(out_reg))\n","\n","def eccv16(pretrained=True):\n","\tmodel = ECCVGenerator()\n","\tif(pretrained):\n","\t\timport torch.utils.model_zoo as model_zoo\n","\t\tmodel.load_state_dict(model_zoo.load_url('https://colorizers.s3.us-east-2.amazonaws.com/colorization_release_v2-9b330a0b.pth',map_location='cpu',check_hash=True))\n","\treturn model\n"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Extractor\n","This is a noval approach to compare to images and use that comparison as a loss to train a model.\n","We will use a pre-trained VGG19 model. Two images are passed in it and the activations of the 18th layers are taken for both the images and then this activations are used to calculate the loss which can be calculated using RMSE, MSE etc between the two activations."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:45.201201Z","iopub.status.busy":"2021-09-07T17:03:45.200852Z","iopub.status.idle":"2021-09-07T17:03:45.206855Z","shell.execute_reply":"2021-09-07T17:03:45.205670Z","shell.execute_reply.started":"2021-09-07T17:03:45.201171Z"},"trusted":true},"outputs":[],"source":["class FeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(FeatureExtractor, self).__init__()\n","        vgg19_model = vgg19(pretrained=True)\n","        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n","\n","    def forward(self, img):\n","        return self.feature_extractor(img)"]},{"cell_type":"markdown","metadata":{},"source":["## Discriminator\n","Takes an image as input and converts it into a single no. after passing through several convolutional blocks."]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:46.483457Z","iopub.status.busy":"2021-09-07T17:03:46.483078Z","iopub.status.idle":"2021-09-07T17:03:46.493703Z","shell.execute_reply":"2021-09-07T17:03:46.492438Z","shell.execute_reply.started":"2021-09-07T17:03:46.483425Z"},"trusted":true},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, input_shape):\n","        super(Discriminator, self).__init__()\n","\n","        self.input_shape = input_shape\n","        in_channels, in_height, in_width = self.input_shape\n","        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n","        self.output_shape = (1, patch_h, patch_w)\n","\n","        def discriminator_block(in_filters, out_filters, first_block=False):\n","            layers = []\n","            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n","            if not first_block:\n","                layers.append(nn.BatchNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n","            layers.append(nn.BatchNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        layers = []\n","        in_filters = in_channels\n","        for i, out_filters in enumerate([64, 128, 256, 512]):\n","            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n","            in_filters = out_filters\n","\n","        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, img):\n","        return self.model(img)"]},{"cell_type":"markdown","metadata":{},"source":["# Image Dataset"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:47.609596Z","iopub.status.busy":"2021-09-07T17:03:47.609268Z","iopub.status.idle":"2021-09-07T17:03:47.773896Z","shell.execute_reply":"2021-09-07T17:03:47.773145Z","shell.execute_reply.started":"2021-09-07T17:03:47.609565Z"},"trusted":true},"outputs":[{"ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Jinay Vora\\OneDrive\\Desktop\\Data_Analytics_ICE\\gan-for-image-colorizatio.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m root \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../input/image-colorization-dataset/data/train_black/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m             [\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                 \u001b[39m#transforms.Resize((shape[0], shape[1]), Image.BICUBIC),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m             ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     ImageDataset(root, transform),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     num_workers\u001b[39m=\u001b[39;49mn_cpu,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Jinay Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:277\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# map-style\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[39mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 277\u001b[0m         sampler \u001b[39m=\u001b[39m RandomSampler(dataset, generator\u001b[39m=\u001b[39;49mgenerator)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Jinay Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:97\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n","\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}],"source":["batch_size = 4\n","n_cpu = 0\n","out_channels = 3\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, root, transform): #, shape):\n","        #height, width = shape\n","        self.transform = transform\n","\n","        self.files = sorted(glob.glob(root + \"/*.*\"))\n","\n","    def __getitem__(self, index):\n","        \n","        black_path = self.files[index % len(self.files)]\n","        color_path = black_path.replace('black','color')\n","        \n","        img_black = np.asarray(Image.open(black_path))\n","        if(img_black.ndim==2):\n","            img_black = np.tile(img_black[:,:,None],3)\n","        (tens_l_orig, tens_l_rs) = preprocess_img(img_black, HW=(400, 400))\n","        #img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n","        #img_black = self.transform(img_black)\n","        \n","        \n","        img_color = Image.open(color_path)\n","        img_color = self.transform(img_color)\n","\n","        return {\"black\": tens_l_rs.squeeze(0), 'orig': tens_l_orig.squeeze(0), \"color\": img_color}\n","\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","root = '../input/image-colorization-dataset/data/train_black/'\n","transform = transforms.Compose(\n","            [\n","                #transforms.Resize((shape[0], shape[1]), Image.BICUBIC),\n","                transforms.ToTensor(),\n","                #transforms.Normalize(mean, std),\n","            ]\n","        )\n","dataloader = DataLoader(\n","    ImageDataset(root, transform),\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=n_cpu,\n",")"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:48.301021Z","iopub.status.busy":"2021-09-07T17:03:48.300736Z","iopub.status.idle":"2021-09-07T17:03:49.480479Z","shell.execute_reply":"2021-09-07T17:03:49.479303Z","shell.execute_reply.started":"2021-09-07T17:03:48.300991Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'dataloader' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Jinay Vora\\OneDrive\\Desktop\\Data_Analytics_ICE\\gan-for-image-colorizatio.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(dataloader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m dataiter\u001b[39m.\u001b[39mnext()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n","\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"]}],"source":["dataiter = iter(dataloader)\n","data = dataiter.next()\n","\n","i = 1\n","img = data['black'][i].permute(1, 2, 0)\n","print(img.shape)\n","t = np.zeros((400, 400, 3))\n","t[..., 0] = img[..., 0]\n","t = color.lab2rgb(t)\n","plt.imshow(t)\n","plt.show()\n","\n","img = data['orig'][i].permute(1, 2, 0)\n","print(img.shape)\n","t = np.zeros((400, 400, 3))\n","t[..., 0] = img[..., 0]\n","t = color.lab2rgb(t)\n","plt.imshow(t)\n","plt.show()\n","\n","print(img.shape)\n","img = data['color'][i].permute(1, 2, 0)\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Initialising Models and Training Params"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:49.482215Z","iopub.status.busy":"2021-09-07T17:03:49.481867Z","iopub.status.idle":"2021-09-07T17:03:49.487687Z","shell.execute_reply":"2021-09-07T17:03:49.486485Z","shell.execute_reply.started":"2021-09-07T17:03:49.482175Z"},"trusted":true},"outputs":[],"source":["class color_ecv(nn.Module):\n","    def __init__(self, in_channels):\n","        super(color_ecv, self).__init__()\n","        \n","        self.model = eccv16(pretrained=True)\n","    \n","    def forward(self, x):\n","        ecv_output = self.model(x)\n","        return ecv_output"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:50.099622Z","iopub.status.busy":"2021-09-07T17:03:50.099154Z","iopub.status.idle":"2021-09-07T17:03:50.104425Z","shell.execute_reply":"2021-09-07T17:03:50.103598Z","shell.execute_reply.started":"2021-09-07T17:03:50.099579Z"},"trusted":true},"outputs":[],"source":["os.makedirs(\"colorit_gan/images\", exist_ok=True)\n","os.makedirs(\"colorit_gan/saved_models\", exist_ok=True)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:51.121362Z","iopub.status.busy":"2021-09-07T17:03:51.121021Z","iopub.status.idle":"2021-09-07T17:03:51.203164Z","shell.execute_reply":"2021-09-07T17:03:51.202216Z","shell.execute_reply.started":"2021-09-07T17:03:51.121333Z"},"trusted":true},"outputs":[],"source":["start_epoch = 0\n","n_epochs= 50\n","\n","lr = 0.0002\n","b1 = 0.5\n","b2 = 0.999\n","decay_epoch = 100\n","in_channels = 1\n","out_channels = 3\n","sample_interval = 100\n","checkpoint_interval = 1\n","\n","cuda = torch.cuda.is_available()"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:03:52.249397Z","iopub.status.busy":"2021-09-07T17:03:52.249064Z","iopub.status.idle":"2021-09-07T17:04:08.144201Z","shell.execute_reply":"2021-09-07T17:04:08.143338Z","shell.execute_reply.started":"2021-09-07T17:03:52.249364Z"},"trusted":true},"outputs":[],"source":["shape = (400, 400)\n","\n","# Initialize generator and discriminator\n","generator = color_ecv(in_channels = 3)\n","discriminator = Discriminator(input_shape=(out_channels, *shape))\n","feature_extractor = FeatureExtractor()\n","\n","# Set feature extractor to inference mode\n","feature_extractor.eval()\n","\n","# Losses\n","criterion_GAN = torch.nn.MSELoss()\n","criterion_content = torch.nn.L1Loss()\n","\n","if cuda:\n","    generator = generator.cuda()\n","    discriminator = discriminator.cuda()\n","    feature_extractor = feature_extractor.cuda()\n","    criterion_GAN = criterion_GAN.cuda()\n","    criterion_content = criterion_content.cuda()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2021-09-07T17:04:08.147706Z","iopub.status.busy":"2021-09-07T17:04:08.147437Z","iopub.status.idle":"2021-09-07T17:04:08.153982Z","shell.execute_reply":"2021-09-07T17:04:08.153190Z","shell.execute_reply.started":"2021-09-07T17:04:08.147679Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# if start_epoch != 0:\n","    # Load pretrained models\n","generator.load_state_dict(torch.load(\"generator_59.pth\" , map_location=torch.device('cpu')))\n","    # discriminator.load_state_dict(torch.load(\"colorit_gan/saved_models/discriminator_\"+str(start_epoch-1)+\".pth\"))\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["torch.save(generator, \"generator_Full.pth\")"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":["color_ecv(\n","  (model): ECCVGenerator(\n","    (model1): Sequential(\n","      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model5): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model6): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model7): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model8): Sequential(\n","      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (softmax): Softmax(dim=1)\n","    (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (upsample4): Upsample(scale_factor=4.0, mode=bilinear)\n","  )\n",")"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["model = torch.load(\"generator_Full.pth\")\n","model.eval()"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["color_ecv(\n","  (model): ECCVGenerator(\n","    (model1): Sequential(\n","      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model5): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model6): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model7): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model8): Sequential(\n","      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (softmax): Softmax(dim=1)\n","    (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (upsample4): Upsample(scale_factor=4.0, mode=bilinear)\n","  )\n",")"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["generator.load_state_dict(torch.load(\"generator_74.pth\" , map_location=torch.device('cpu')))\n","torch.save(generator, \"generator_74_Full.pth\")\n","model = torch.load(\"generator_74_Full.pth\")\n","model.eval()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["color_ecv(\n","  (model): ECCVGenerator(\n","    (model1): Sequential(\n","      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model5): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model6): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model7): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model8): Sequential(\n","      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (softmax): Softmax(dim=1)\n","    (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (upsample4): Upsample(scale_factor=4.0, mode=bilinear)\n","  )\n",")"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["generator.load_state_dict(torch.load(\"generator_89.pth\" , map_location=torch.device('cpu')))\n","torch.save(generator, \"generator_89_Full.pth\")\n","model = torch.load(\"generator_89_Full.pth\")\n","model.eval()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'color_ecv' object has no attribute 'save'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Jinay Vora\\OneDrive\\Desktop\\Data_Analytics_ICE\\gan-for-image-colorizatio.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generator\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mgenerator_89.pth\u001b[39m\u001b[39m\"\u001b[39m , map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_scripted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49msave(generator, \u001b[39m\"\u001b[39;49m\u001b[39mgenerator90_jitted.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n","File \u001b[1;32mc:\\Users\\Jinay Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\jit\\_serialization.py:81\u001b[0m, in \u001b[0;36msave\u001b[1;34m(m, f, _extra_files)\u001b[0m\n\u001b[0;32m     79\u001b[0m     _extra_files \u001b[39m=\u001b[39m {}\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(f, pathlib\u001b[39m.\u001b[39mPath):\n\u001b[1;32m---> 81\u001b[0m     m\u001b[39m.\u001b[39;49msave(f, _extra_files\u001b[39m=\u001b[39m_extra_files)\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     ret \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39msave_to_buffer(_extra_files\u001b[39m=\u001b[39m_extra_files)\n","File \u001b[1;32mc:\\Users\\Jinay Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n","\u001b[1;31mAttributeError\u001b[0m: 'color_ecv' object has no attribute 'save'"]}],"source":["generator.load_state_dict(torch.load(\"generator_89.pth\" , map_location=torch.device('cpu')))\n","model_scripted = torch.jit.save(generator, \"generator90_jitted.pth\") # Export to TorchScript\n","# model_scripted.save('generator90_scripted.pt') # Save"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["color_ecv(\n","  (model): ECCVGenerator(\n","    (model1): Sequential(\n","      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model5): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model6): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model7): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model8): Sequential(\n","      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (softmax): Softmax(dim=1)\n","    (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (upsample4): Upsample(scale_factor=4.0, mode=bilinear)\n","  )\n",")"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["generator.eval()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["import cv2\n","def load(path,shape):\n","    img= cv2.imread(path)\n","    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img= cv2.resize(img, shape)\n","    return img\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["import numpy as np\n","import glob\n","import os\n","import matplotlib.pyplot as plt \n","from PIL import Image\n","import torch\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def get_data(path):\n","    X=[]\n","    Y=[]\n","    for folder in glob.glob(path+ str('/*')):\n","        for img_path in glob.glob(folder+ str('/*')):\n","            if folder == os.path.join(path, 'test_black'):\n","                X.append(load(img_path, (400, 400)))\n","            elif folder == os.path.join(path, 'test_color'):\n","                Y.append(load(img_path, (400,400)))\n","    X= np.array(X)\n","    Y= np.array(Y)\n","    return X/255.0, Y/255.0\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["test_black , test_color = get_data(\"data\")\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":["(10, 400, 400, 3)"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["test_black.shape"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["(10, 400, 400, 3)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["test_color.shape"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["color_ecv(\n","  (model): ECCVGenerator(\n","    (model1): Sequential(\n","      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model5): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model6): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model7): Sequential(\n","      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (model8): Sequential(\n","      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(256, 313, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (softmax): Softmax(dim=1)\n","    (model_out): Conv2d(313, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (upsample4): Upsample(scale_factor=4.0, mode=bilinear)\n","  )\n",")"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["model = torch.load(\"generator_60_Full.pth\")\n","model.eval()"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"unsupported operand type(s) for -: 'Image' and 'float'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Jinay Vora\\OneDrive\\Desktop\\Data_Analytics_ICE\\gan-for-image-colorizatio.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m img\u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(np\u001b[39m.\u001b[39muint8(test_black[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m255\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m img2 \u001b[39m=\u001b[39m model(img)\n","File \u001b[1;32mc:\\Users\\Jinay Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\Users\\Jinay Vora\\OneDrive\\Desktop\\Data_Analytics_ICE\\gan-for-image-colorizatio.ipynb Cell 35\u001b[0m in \u001b[0;36mcolor_ecv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ecv_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ecv_output\n","File \u001b[1;32mc:\\Users\\Jinay Vora\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\Users\\Jinay Vora\\OneDrive\\Desktop\\Data_Analytics_ICE\\gan-for-image-colorizatio.ipynb Cell 35\u001b[0m in \u001b[0;36mECCVGenerator.forward\u001b[1;34m(self, input_l)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_l):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     conv1_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize_l(input_l))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     conv2_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel2(conv1_2)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     conv3_3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel3(conv2_2)\n","\u001b[1;32mc:\\Users\\Jinay Vora\\OneDrive\\Desktop\\Data_Analytics_ICE\\gan-for-image-colorizatio.ipynb Cell 35\u001b[0m in \u001b[0;36mBaseColor.normalize_l\u001b[1;34m(self, in_l)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_l\u001b[39m(\u001b[39mself\u001b[39m, in_l):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jinay%20Vora/OneDrive/Desktop/Data_Analytics_ICE/gan-for-image-colorizatio.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \t\u001b[39mreturn\u001b[39;00m (in_l\u001b[39m-\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_cent)\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_norm\n","\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Image' and 'float'"]}],"source":["img= Image.fromarray(np.uint8(test_black[0]*255))\n","img2 = model(img)\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'collections.OrderedDict' object is not callable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X50sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m input_image \u001b[39m=\u001b[39m transforms(input_image)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X50sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# generate the output image using the loaded model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X50sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m output_image \u001b[39m=\u001b[39m model(input_image)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X50sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# create a PIL image from the output tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m output_image \u001b[39m=\u001b[39m (output_image \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"]}],"source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","\n","# define the location of the pre-trained model file\n","model_path = 'generator_60_Full.pth'\n","\n","# load the model\n","model = torch.load(model_path, map_location=torch.device('cpu'))\n","\n","# define the input image size\n","input_size = 400\n","\n","# define the transforms to apply to the input image\n","transforms = transforms.Compose([\n","    transforms.Resize(input_size),\n","    transforms.ToTensor()\n","])\n","\n","# load the input image and apply the transforms\n","input_image_path = 'data/test_black/image5005.jpg'\n","input_image = Image.open(input_image_path).convert('L')\n","input_image = transforms(input_image).unsqueeze(0)\n","\n","# generate the output image using the loaded model\n","output_image = model(input_image).squeeze().detach().numpy()\n","\n","# create a PIL image from the output tensor\n","output_image = (output_image * 255).astype('uint8')\n","output_image = Image.fromarray(output_image, mode='L')\n","\n","# save the output image as a JPEG file\n","output_image_path = 'output.jpg'\n","output_image.save(output_image_path)\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'collections.OrderedDict' object has no attribute 'eval'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# load the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_path, map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# load the input image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harrymacbook/Documents/VisualCodes/ICE_MP/gan-for-image-colorizatio.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m input_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39minput_image.jpg\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"]}],"source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","\n","# load the model\n","model = torch.load(model_path, map_location=torch.device('cpu'))\n","model.eval()\n","\n","# load the input image\n","input_image = Image.open('input_image.jpg').convert('L')\n","\n","# define the transformations to apply to the input image\n","transforms = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","# apply the transformations to the input image\n","input_image = transforms(input_image).unsqueeze(0)\n","\n","# generate the output image using the loaded model\n","output_image = model(input_image)['output'].squeeze().detach().numpy()\n","\n","# create a PIL image from the output tensor\n","output_image = (output_image * 255).astype('uint8')\n","output_image = Image.fromarray(output_image)\n","\n","# save the output image as a JPEG file\n","output_image.save('output_image.jpg')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"},"vscode":{"interpreter":{"hash":"8f0063560737609a141aa87b39577853acdfc9932e5c5196f0acdb90df11be1d"}}},"nbformat":4,"nbformat_minor":4}
